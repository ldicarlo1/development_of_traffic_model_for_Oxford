{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "tracked-importance",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "selected-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change directory to parent folder to access all folders\n",
    "import os\n",
    "path = os.path.dirname(os.getcwd())\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "crude-speech",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense,concatenate,Reshape\n",
    "from stellargraph import StellarGraph, StellarDiGraph\n",
    "import stellargraph as sg\n",
    "from datetime import datetime\n",
    "from stellargraph.layer import GCN_LSTM\n",
    "import pmdarima as pm\n",
    "from pmdarima.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from classes import model_performance,preprocessing, load_traffic_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-luther",
   "metadata": {},
   "source": [
    "## Load Traffic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "frequent-librarian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of road segments within the area of interest is:  70\n"
     ]
    }
   ],
   "source": [
    "# Peartree roundabout bbox and datetimes of interest\n",
    "top=51.798433\n",
    "bottom=51.791451\n",
    "right=-1.281979\n",
    "left=-1.289524\n",
    "datetime_start=datetime(2021,6,23,0,0)\n",
    "datetime_end=datetime(2021,7,13,10,50)\n",
    "\n",
    "# load in traffic data\n",
    "traffic_data,time = load_traffic_data.Import_Traffic_Data(top,bottom,right,left).load_traffic_data(datetime_start,datetime_end, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "studied-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# speed capped by speed limit\n",
    "sp = traffic_data[:,:,5]\n",
    "\n",
    "# coordinates\n",
    "lons = traffic_data[0,:,4]\n",
    "lats = traffic_data[0,:,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-uniform",
   "metadata": {},
   "source": [
    "## Load WX data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "exposed-yugoslavia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in 5min wx data from csv\n",
    "wx_df = pd.read_csv(\"data_collection/data/wx_data/oxfordcity_wx_variables_5min_intervals.csv\")\n",
    "\n",
    "# collect variables of significance\n",
    "wx_vars = wx_df[['precipitationIntensity','temperature','humidity','weatherCode']]\n",
    "\n",
    "wx_vars_scaled = np.zeros_like(wx_vars)\n",
    "\n",
    "# normalize between 0 and 1\n",
    "for i in range(4):\n",
    "    norm = (wx_vars.iloc[:,i] - wx_vars.iloc[:,i].min())/(wx_vars.iloc[:,i].max() - wx_vars.iloc[:,i].min())\n",
    "    wx_vars_scaled[:,i] = norm\n",
    "    #print(wx_vars_scaled[i].max())\n",
    "    \n",
    "    \n",
    "# transpose data to be in proper format for preprocessing   \n",
    "wx_vars_scaled = wx_vars_scaled.T\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-seeker",
   "metadata": {},
   "source": [
    "## Create Road-Network Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ready-subsection",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# load in csv of node connections\n",
    "connections = pd.read_csv(f\"{path}/data_preprocessing/peartree_roundabout.csv\")\n",
    "\n",
    "# convert feeding roads to integers\n",
    "for i in range(len(connections)):\n",
    "#for i in range(4):\n",
    "    try:\n",
    "        connections.feeding_roads.iloc[i] = ast.literal_eval(connections.feeding_roads.iloc[i])\n",
    "    except ValueError:\n",
    "        connections.feeding_roads.iloc[i] = np.nan\n",
    "\n",
    "# node connections\n",
    "nodes = connections[\"Unnamed: 0\"]\n",
    "roads = connections.feeding_roads\n",
    "\n",
    "# replace nans with 0's\n",
    "connections.feeding_roads = connections.feeding_roads.fillna(0)\n",
    "\n",
    "# loop thru and establish edges\n",
    "edge_list = []\n",
    "for row in range(len(roads)):\n",
    "    node1 = connections[\"Unnamed: 0\"].iloc[row]\n",
    "    node2 = connections.feeding_roads.iloc[row]\n",
    "    try:\n",
    "        for i in range(len(node2)):\n",
    "            edge_list.append([node2[i], node1])\n",
    "        #node2 = connections.feeding_roads.iloc[row]\n",
    "    except TypeError:\n",
    "        edge_list.append([node2, node1])\n",
    "        \n",
    "# remove 0's\n",
    "edges = []\n",
    "for edge in edge_list:\n",
    "    if edge[0]==0:\n",
    "        pass\n",
    "    else:\n",
    "        edges.append(edge)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-haiti",
   "metadata": {},
   "source": [
    "###### Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "integrated-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "for i in range(len(nodes)):\n",
    "    G.add_node(nodes[i],spd=sp[:,i])\n",
    "    #G.add_edge(nodes[i])\n",
    "#G.add_nodes_from(nodes)\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# get adjacency matrix \n",
    "A = nx.to_numpy_array(G)\n",
    "\n",
    "# convert graph to stellargraph object for modeling\n",
    "square = StellarGraph.from_networkx(G,node_features=\"spd\")\n",
    "\n",
    "# get feature matrix\n",
    "X = square.node_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-parks",
   "metadata": {},
   "source": [
    "## Build T-GCN Model\n",
    "\n",
    "###### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "smoking-parks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:  (70, 4712)\n",
      "Test data:  (70, 1179)\n",
      "WX Train data:  (4, 4712)\n",
      "WX Test data:  (4, 1179)\n"
     ]
    }
   ],
   "source": [
    "# specify the training rate\n",
    "train_rate = 0.8\n",
    "\n",
    "# replace missing values with nans\n",
    "X = np.where(X<0,0,X)\n",
    "\n",
    "# split train/test\n",
    "train_data, test_data = preprocessing.train_test_split(X, train_rate)\n",
    "wx_train_data, wx_test_data = preprocessing.train_test_split(wx_vars_scaled, train_rate)\n",
    "\n",
    "print(\"Train data: \", train_data.shape)\n",
    "print(\"Test data: \", test_data.shape)\n",
    "print(\"WX Train data: \", wx_train_data.shape)\n",
    "print(\"WX Test data: \", wx_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-endorsement",
   "metadata": {},
   "source": [
    "###### Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "contrary-intelligence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data based on max/min\n",
    "train_scaled, test_scaled = preprocessing.scale_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-portrait",
   "metadata": {},
   "source": [
    "##### Set weather parameters for each node of road network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bridal-coast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new train/test variables for wx variables\n",
    "wx_train_data_ = []\n",
    "wx_test_data_ = []\n",
    "\n",
    "# loop thru and assign the wx data to each node\n",
    "for i in range(70):\n",
    "    wx_train_data_.append(wx_train_data.T)\n",
    "    wx_test_data_.append(wx_test_data.T)\n",
    "\n",
    "# convert data to correct shape\n",
    "wx_train_data_ = np.array(wx_train_data_)\n",
    "wx_test_data_ = np.array(wx_test_data_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-vector",
   "metadata": {},
   "source": [
    "###### Pre-process data based on sequence and prediction length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "mexican-cycling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4700, 70, 12)\n",
      "(4700, 70)\n",
      "(1167, 70, 12)\n",
      "(1167, 70)\n",
      "(4700, 70, 12, 4)\n",
      "(4700, 70, 4)\n",
      "(1167, 70, 12, 4)\n",
      "(1167, 70, 4)\n"
     ]
    }
   ],
   "source": [
    "# the number of timesteps up to the prediction that we will feed to the model (5-minute intervals)\n",
    "seq_len = 12\n",
    "\n",
    "# the amount of time in advance we want to predict (5-minute intervals)\n",
    "pre_len = 1\n",
    "\n",
    "# preprocess traffic data\n",
    "traffic_trainX, trainY, traffic_testX, testY = preprocessing.sequence_data_preparation(\n",
    "    seq_len, pre_len, train_scaled, test_scaled\n",
    ")\n",
    "\n",
    "# preprocessing weather data\n",
    "wx_trainX, wx_trainY, wx_testX, wx_testY = preprocessing.sequence_data_preparation(\n",
    "    seq_len, pre_len, wx_train_data_, wx_test_data_\n",
    ")\n",
    "print(traffic_trainX.shape)\n",
    "print(trainY.shape)\n",
    "print(traffic_testX.shape)\n",
    "print(testY.shape)\n",
    "print(wx_trainX.shape)\n",
    "print(wx_trainY.shape)\n",
    "print(wx_testX.shape)\n",
    "print(wx_testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aging-costs",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Combine the weather variables w/ traffic matrix to create feature matrix\n",
    "trainX = np.empty((len(wx_trainX[:,0,0,0]),len(wx_trainX[0,:,0,0]),len(wx_trainX[0,0,:,0]),5) )\n",
    "testX = np.empty((len(wx_testX[:,0,0,0]),len(wx_testX[0,:,0,0]),len(wx_testX[0,0,:,0]),5) )\n",
    "\n",
    "trainX[:,:,:,0] = traffic_trainX\n",
    "trainX[:,:,:,1:5] = wx_trainX\n",
    "testX[:,:,:,0] = traffic_testX\n",
    "testX[:,:,:,1:5] = wx_testX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-attack",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "imported-pepper",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Below hyperparameter tuning will be performed layer by layer using varying activation functions and layer sizes. Each layer will be optimized and then a following layer will be tuned. This will be performed on a model using NO WX DATA first. After tuning will be performed on a model incorporating weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "elder-rally",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "\n",
    "class TGCN_LSTM_hyperparameter_tuning():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def hyperparameters(self, batch_size, optimizer, loss, metrics, epochs):\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "        self.metrics = metrics\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def graph_conv_layers(self, gc_layer_size, gc_layer_activation):\n",
    "        self.gc_layer_size = gc_layer_size\n",
    "        self.gc_layer_activation = gc_layer_activation\n",
    "        \n",
    "        print(f\"GCN Layer Sizes: {self.gc_layer_size}\")\n",
    "        print(f\"GCN Layer Activations: {self.gc_layer_activation}\")\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    def lstm_layers(self, lstm_layer_size, lstm_layer_activation):\n",
    "        self.lstm_layer_size = lstm_layer_size\n",
    "        self.lstm_layer_activation = lstm_layer_activation\n",
    "        \n",
    "        print(f\"LSTM Layer Sizes: {self.lstm_layer_size}\")\n",
    "        print(f\"LSTM Layer Activations: {self.lstm_layer_activation}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def _model_perf(self,trainX,trainY,testX,testY,gc_layer_size, gc_layer_activation,lstm_layer_size,lstm_layer_activation):\n",
    "        '''Function tunes the hyperparameters of the GCN-LSTM Model.\n",
    "        '''\n",
    "        \n",
    "        gcn_lstm = GCN_LSTM(\n",
    "            seq_len=seq_len,\n",
    "            adj=A,\n",
    "            gc_layer_sizes=[30,30,gc_layer_size],\n",
    "            gc_activations=[\"relu\",gc_layer_activation],\n",
    "            lstm_layer_sizes=[400,400,lstm_layer_size],\n",
    "            lstm_activations=[\"tanh\",lstm_layer_activation],\n",
    "        )\n",
    "        # model architecture with keras\n",
    "        x_input, x_output = gcn_lstm.in_out_tensors()\n",
    "        model = Model(inputs=x_input, outputs=x_output)\n",
    "\n",
    "        # compile model\n",
    "        model.compile(optimizer=self.optimizer, loss=self.loss, metrics=self.metrics)\n",
    "\n",
    "        # train model\n",
    "        history = model.fit(\n",
    "            trainX,\n",
    "            trainY,\n",
    "            epochs=self.epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            verbose=0,\n",
    "            validation_data=(testX,testY)\n",
    "        )\n",
    "\n",
    "        # get model predictions\n",
    "        pred = model.predict(testX).T\n",
    "\n",
    "        mse = model_performance.spatio_temporal_traffic_model(lr).mse(testY,pred).mean()\n",
    "        mae = model_performance.spatio_temporal_traffic_model(lr).mae(testY,pred).mean()\n",
    "        r2 =  model_performance.spatio_temporal_traffic_model(lr).r2(testY,pred).mean()\n",
    "\n",
    "        return mse, mae, r2\n",
    "\n",
    "    def validate(self, trainX,trainY,testX,testY):\n",
    "        # fill results into empty list\n",
    "        results = []\n",
    "        activations = []\n",
    "        lstmactivations=[]\n",
    "        gclayers = []\n",
    "        lstmlayers=[]\n",
    "\n",
    "        # loop thru each varying layer size and activation function\n",
    "        for activation in self.gc_layer_activation:\n",
    "            for lstmactivation in self.lstm_layer_activation:  \n",
    "                for gcsize in self.gc_layer_size:\n",
    "                    for lstmsize in self.lstm_layer_size:\n",
    "                        results.append(self._model_perf(trainX,trainY,testX,testY,gcsize,activation,lstmsize,lstmactivation))\n",
    "                        activations.append(activation)\n",
    "                        lstmactivations.append(lstmactivation)\n",
    "                        gclayers.append(gcsize)\n",
    "                        lstmlayers.append(lstmsize)\n",
    "                        print(activation, gcsize, lstmactivation,lstmsize)\n",
    "        \n",
    "        # create dataframe of results\n",
    "        results = np.array(results)\n",
    "        results = pd.DataFrame({\"gcn_activation\":activations[:],\"gc_layer_size\":gclayers[:],\"lstm_activation\":lstmactivations[:],\"lstm_layer_size\":lstmlayers[:],\"mse\":results[:,0],\"mae\":results[:,1],\"r2\":results[:,2]})\n",
    "        \n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-belly",
   "metadata": {},
   "source": [
    "##### Tune the model layer by layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-light",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune = TGCN_LSTM_hyperparameter_tuning()\n",
    "tune.graph_conv_layers(gc_layer_size=[5,10,15],gc_layer_activation=[\"relu\"])\n",
    "tune.lstm_layers([100,200,300,400],[\"tanh\"])\n",
    "tune.hyperparameters(batch_size=64,loss=\"mse\",metrics=[\"mse\"],epochs=20,optimizer=\"adam\")\n",
    "results = tune.validate(trainX, trainY, testX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-murder",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"modeling/hyperparameter_tuning/t-gcn_layer2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-addiction",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "#### Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-terrace",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = pd.read_csv(\"modeling/hyperparameter_tuning/t-gcn_layer1.csv\")\n",
    "layer2 = pd.read_csv(\"modeling/hyperparameter_tuning/t-gcn_layer2.csv\")\n",
    "layer3 = pd.read_csv(\"modeling/hyperparameter_tuning/t-gcn_layer3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-commercial",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = sns.catplot(data=layer1,x=\"gc_layer_size\",y=\"mse\",col=\"gcn_activation\",kind=\"bar\")\n",
    "cp.fig.subplots_adjust(top=0.8)\n",
    "cp.fig.suptitle('Layer 1 GCN Hyperparameter Tuning Results')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-calculator",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = sns.catplot(data=layer1,x=\"lstm_layer_size\",y=\"mae\",col=\"lstm_activation\",kind=\"bar\")\n",
    "cp.fig.subplots_adjust(top=0.8)\n",
    "cp.fig.suptitle('Layer 1 LSTM Hyperparameter Tuning Results')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-quarterly",
   "metadata": {},
   "source": [
    "#### Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-spending",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = sns.catplot(data=layer2,x=\"gc_layer_size\",y=\"mse\",col=\"gcn_activation\",kind=\"bar\")\n",
    "cp.fig.subplots_adjust(top=0.8)\n",
    "cp.fig.suptitle('Layer 2 GCN Hyperparameter Tuning Results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-freeware",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = sns.catplot(data=layer2,x=\"lstm_layer_size\",y=\"mae\",col=\"lstm_activation\",kind=\"bar\")\n",
    "cp.fig.subplots_adjust(top=0.8)\n",
    "cp.fig.suptitle('Layer 2 LSTM Hyperparameter Tuning Results')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-comparative",
   "metadata": {},
   "source": [
    "##### Layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = sns.catplot(data=layer3,x=\"gc_layer_size\",y=\"mse\",col=\"gcn_activation\",kind=\"bar\")\n",
    "cp.fig.subplots_adjust(top=0.8)\n",
    "cp.fig.suptitle('Layer 3 GCN Hyperparameter Tuning Results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-anatomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = sns.catplot(data=layer3,x=\"lstm_layer_size\",y=\"mae\",col=\"lstm_activation\",kind=\"bar\")\n",
    "cp.fig.subplots_adjust(top=0.8)\n",
    "cp.fig.suptitle('Layer 3 LSTM Hyperparameter Tuning Results')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-insider",
   "metadata": {},
   "source": [
    "Two layers should suffice as the error does not decrease significantly using 3 layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-decrease",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning: T-GCN-wx\n",
    "\n",
    "Tune the T-GCN with weather data incorporated as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "\n",
    "class TGCN_LSTM_WX_hyperparameter_tuning():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def hyperparameters(self, batch_size, optimizer, loss, metrics, epochs):\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "        self.metrics = metrics\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def graph_conv_layers(self, gc_layer_size, gc_layer_activation):\n",
    "        self.gc_layer_size = gc_layer_size\n",
    "        self.gc_layer_activation = gc_layer_activation\n",
    "        \n",
    "        print(f\"GCN Layer Sizes: {self.gc_layer_size}\")\n",
    "        print(f\"GCN Layer Activations: {self.gc_layer_activation}\")\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    def lstm_layers(self, lstm_layer_size, lstm_layer_activation):\n",
    "        self.lstm_layer_size = lstm_layer_size\n",
    "        self.lstm_layer_activation = lstm_layer_activation\n",
    "        \n",
    "        print(f\"LSTM Layer Sizes: {self.lstm_layer_size}\")\n",
    "        print(f\"LSTM Layer Activations: {self.lstm_layer_activation}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def data_fusion_activation(self, data_fusion_activation):\n",
    "        self.data_fusion_activation = data_fusion_activation\n",
    "        \n",
    "        print(f\"Data Fusion Layer Activations: {self.data_fusion_activation}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def _model_perf(self,trainX,trainY,testX,testY,gc_layer_size, gc_layer_activation,lstm_layer_size,lstm_layer_activation,data_fusion_activation):\n",
    "        '''Function tunes the hyperparameters of the GCN-LSTM_WX Model.\n",
    "        '''\n",
    "        \n",
    "        gcn_lstm = GCN_LSTM(\n",
    "            seq_len=seq_len,\n",
    "            adj=A,\n",
    "            gc_layer_sizes=[5,45,gc_layer_size],\n",
    "            gc_activations=[\"relu\",\"tanh\",gc_layer_activation],\n",
    "            lstm_layer_sizes=[400,100,lstm_layer_size],\n",
    "            lstm_activations=[\"tanh\",\"relu\",lstm_layer_activation],\n",
    "        )\n",
    "\n",
    "        # build data fusion layer which will merge wx/traffic feature matrix (length 5) into one array to be fed into the t-gcn model\n",
    "        input_layer = Input(shape=(70,12,5))\n",
    "        layer1 = Dense(1, activation=data_fusion_activation)(input_layer)\n",
    "        output_layer = Reshape((70,12))(layer1)\n",
    "        data_fusion_model = Model(input_layer,output_layer)\n",
    "\n",
    "        # recall tgcn model\n",
    "        x_input, x_output = gcn_lstm.in_out_tensors()\n",
    "        tgcn_model = Model(inputs=x_input, outputs=x_output)\n",
    "\n",
    "\n",
    "        # of data fusion model feeds into t-gcn\n",
    "        output = tgcn_model(data_fusion_model.output)\n",
    "\n",
    "        # define entire model \n",
    "        tgcn_wx_model = Model(data_fusion_model.input,output, name=\"T-GCN-WX\")\n",
    "        #tgcn_wx_model.summary()\n",
    "\n",
    "        # compile model\n",
    "        tgcn_wx_model.compile(optimizer=\"adam\", loss=\"mae\", metrics=[\"mse\"])\n",
    "\n",
    "        # train model\n",
    "        history = tgcn_wx_model.fit(\n",
    "            trainX,\n",
    "            trainY,\n",
    "            epochs=self.epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            verbose=0,\n",
    "            validation_data=(trainX,trainY)\n",
    "        )\n",
    "\n",
    "        # get model predictions\n",
    "        pred = tgcn_wx_model.predict(testX).T\n",
    "\n",
    "        mse = model_performance.spatio_temporal_traffic_model(lr).mse(testY,pred).mean()\n",
    "        mae = model_performance.spatio_temporal_traffic_model(lr).mae(testY,pred).mean()\n",
    "        r2 =  model_performance.spatio_temporal_traffic_model(lr).r2(testY,pred).mean()\n",
    "\n",
    "        return mse, mae, r2\n",
    "\n",
    "    def validate(self, trainX,trainY,testX,testY):\n",
    "        # fill results into empty list\n",
    "        results = []\n",
    "        activations = []\n",
    "        lstmactivations=[]\n",
    "        gclayers = []\n",
    "        lstmlayers=[]\n",
    "        datafusionactivations=[]\n",
    "\n",
    "        # loop thru each varying layer size and activation function\n",
    "        for datafusionactivation in self.data_fusion_activation:     \n",
    "            for activation in self.gc_layer_activation:\n",
    "                for lstmactivation in self.lstm_layer_activation:  \n",
    "                    for gcsize in self.gc_layer_size:\n",
    "                        for lstmsize in self.lstm_layer_size:\n",
    "                            results.append(self._model_perf(trainX,trainY,testX,testY,gcsize,activation,lstmsize,lstmactivation,datafusionactivation))\n",
    "                            activations.append(activation)\n",
    "                            lstmactivations.append(lstmactivation)\n",
    "                            gclayers.append(gcsize)\n",
    "                            lstmlayers.append(lstmsize)\n",
    "                            datafusionactivations.append(datafusionactivation)\n",
    "                            print(datafusionactivation,activation, gcsize, lstmactivation,lstmsize)\n",
    "        \n",
    "        # create dataframe of results\n",
    "        results = np.array(results)\n",
    "        results = pd.DataFrame({\"data_fusion_activation\":datafusionactivations[:],\"gcn_activation\":activations[:],\"gc_layer_size\":gclayers[:],\"lstm_activation\":lstmactivations[:],\"lstm_layer_size\":lstmlayers[:],\"mse\":results[:,0],\"mae\":results[:,1],\"r2\":results[:,2]})\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-lawrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune = TGCN_LSTM_WX_hyperparameter_tuning()\n",
    "tune.data_fusion_activation(data_fusion_activation=[\"linear\"])\n",
    "tune.graph_conv_layers(gc_layer_size=[5,15,30,45],gc_layer_activation=[\"relu\",\"tanh\",\"sigmoid\"])\n",
    "tune.lstm_layers([100,200,300,400],[\"relu\",\"tanh\",\"sigmoid\"])\n",
    "tune.hyperparameters(batch_size=64,loss=\"mae\",metrics=[\"mse\"],epochs=20,optimizer=\"adam\")\n",
    "results_wx = tune.validate(trainX, trainY, testX, testY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-second",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_wx.to_csv(\"modeling/hyperparameter_tuning/t-gcn_wx_layer2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-powell",
   "metadata": {},
   "source": [
    "#### Data Fusion Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-reply",
   "metadata": {},
   "outputs": [],
   "source": [
    "datafusion_layer = pd.read_csv(\"modeling/hyperparameter_tuning/t-gcn_wx_datafusion_layer.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-matrix",
   "metadata": {},
   "source": [
    "###### Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-cincinnati",
   "metadata": {},
   "outputs": [],
   "source": [
    "wxlayer1 = pd.read_csv(\"modeling/hyperparameter_tuning/t-gcn_wx_layer1.csv\")\n",
    "wxlayer2 = pd.read_csv(\"modeling/hyperparameter_tuning/t-gcn_wx_layer2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-stand",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = sns.catplot(data=datafusion_layer,x=\"data_fusion_activation\",y=\"mse\",kind=\"bar\")\n",
    "cp.fig.subplots_adjust(top=0.8)\n",
    "cp.fig.suptitle('Data Fusion: Hyperparameter Tuning Results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-school",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = sns.catplot(data=wxlayer1,x=\"gc_layer_size\",y=\"mse\",col=\"gcn_activation\",kind=\"bar\")\n",
    "cp.fig.subplots_adjust(top=0.8)\n",
    "cp.fig.suptitle('Layer 1 GCN Hyperparameter Tuning Results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-trailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = sns.catplot(data=wxlayer1,x=\"lstm_layer_size\",y=\"mse\",col=\"lstm_activation\",kind=\"bar\")\n",
    "cp.fig.subplots_adjust(top=0.8)\n",
    "cp.fig.suptitle('Layer 1 LSTM Hyperparameter Tuning Results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-laundry",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = sns.catplot(data=wxlayer2,x=\"gc_layer_size\",y=\"mse\",col=\"gcn_activation\",kind=\"bar\")\n",
    "cp.fig.subplots_adjust(top=0.8)\n",
    "cp.fig.suptitle('Layer 2 GCN Hyperparameter Tuning Results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-factory",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = sns.catplot(data=wxlayer2,x=\"lstm_layer_size\",y=\"mse\",col=\"lstm_activation\",kind=\"bar\")\n",
    "cp.fig.subplots_adjust(top=0.8)\n",
    "cp.fig.suptitle('Layer 2 LSTM Hyperparameter Tuning Results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-celebrity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-gabriel",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
