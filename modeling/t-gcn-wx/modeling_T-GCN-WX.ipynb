{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "educational-browse",
   "metadata": {},
   "source": [
    "# Modeling of Peartree Roundabout Traffic Using WX Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-marina",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change directory to parent folder to access all folders\n",
    "import os\n",
    "path = os.path.dirname(os.getcwd())\n",
    "os.chdir(path)\n",
    "from data_preprocessing.classes.load_traffic_data import Import_Traffic_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-community",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense,concatenate,Reshape,BatchNormalization\n",
    "from stellargraph import StellarGraph, StellarDiGraph\n",
    "import stellargraph as sg\n",
    "from datetime import datetime\n",
    "from stellargraph.layer import GCN_LSTM\n",
    "import pmdarima as pm\n",
    "from pmdarima.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from classes import model_performance,preprocessing\n",
    "import pickle\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-exemption",
   "metadata": {},
   "source": [
    "## Load Traffic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-stationery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peartree roundabout bbox and datetimes of interest\n",
    "top=51.798433\n",
    "bottom=51.791451\n",
    "right=-1.281979\n",
    "left=-1.289524\n",
    "datetime_start=datetime(2021,6,23,0,0)\n",
    "datetime_end=datetime(2021,7,13,10,50)\n",
    "\n",
    "# load in traffic data\n",
    "traffic_data,time = Import_Traffic_Data(top,bottom,right,left).load_traffic_data(datetime_start,datetime_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-bonus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# speed capped by speed limit\n",
    "sp = traffic_data[:,:,5]\n",
    "\n",
    "# coordinates\n",
    "lons = traffic_data[0,:,4]\n",
    "lats = traffic_data[0,:,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-celebration",
   "metadata": {},
   "source": [
    "## Load WX data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-bonus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in 5min wx data from csv\n",
    "wx_df = pd.read_csv(\"data_collection/data/wx_data/oxfordcity_wx_variables_5min_intervals.csv\")\n",
    "\n",
    "# collect variables of significance\n",
    "wx_vars = wx_df[['precipitationIntensity','temperature','humidity','weatherCode']]\n",
    "\n",
    "wx_vars_scaled = np.zeros_like(wx_vars)\n",
    "\n",
    "# normalize between 0 and 1\n",
    "for i in range(4):\n",
    "    norm = (wx_vars.iloc[:,i] - wx_vars.iloc[:,i].min())/(wx_vars.iloc[:,i].max() - wx_vars.iloc[:,i].min())\n",
    "    wx_vars_scaled[:,i] = norm\n",
    "    #print(wx_vars_scaled[i].max())\n",
    "    \n",
    "    \n",
    "# transpose data to be in proper format for preprocessing   \n",
    "wx_vars_scaled = wx_vars_scaled.T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-strike",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "stupid-borough",
   "metadata": {},
   "source": [
    "## Create Road-Network Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-therapy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in csv of node connections\n",
    "connections = pd.read_csv(f\"{path}/data_preprocessing/peartree_roundabout.csv\")\n",
    "\n",
    "# convert feeding roads to integers\n",
    "for i in range(len(connections)):\n",
    "#for i in range(4):\n",
    "    try:\n",
    "        connections.feeding_roads.iloc[i] = ast.literal_eval(connections.feeding_roads.iloc[i])\n",
    "    except ValueError:\n",
    "        connections.feeding_roads.iloc[i] = np.nan\n",
    "\n",
    "# node connections\n",
    "nodes = connections[\"Unnamed: 0\"]\n",
    "roads = connections.feeding_roads\n",
    "\n",
    "# replace nans with 0's\n",
    "connections.feeding_roads = connections.feeding_roads.fillna(0)\n",
    "\n",
    "# loop thru and establish edges\n",
    "edge_list = []\n",
    "for row in range(len(roads)):\n",
    "    node1 = connections[\"Unnamed: 0\"].iloc[row]\n",
    "    node2 = connections.feeding_roads.iloc[row]\n",
    "    try:\n",
    "        for i in range(len(node2)):\n",
    "            edge_list.append([node2[i], node1])\n",
    "        #node2 = connections.feeding_roads.iloc[row]\n",
    "    except TypeError:\n",
    "        edge_list.append([node2, node1])\n",
    "        \n",
    "# remove 0's\n",
    "edges = []\n",
    "for edge in edge_list:\n",
    "    if edge[0]==0:\n",
    "        pass\n",
    "    else:\n",
    "        edges.append(edge)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-democrat",
   "metadata": {},
   "source": [
    "##### Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-amsterdam",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "for i in range(len(nodes)):\n",
    "    G.add_node(nodes[i],spd=sp[:,i],wx=wx_vars_scaled[:,0].T)\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# get adjacency matrix \n",
    "A = nx.to_numpy_array(G)\n",
    "\n",
    "# convert graph to stellargraph object for modeling\n",
    "square = StellarGraph.from_networkx(G,node_features=\"spd\")\n",
    "\n",
    "# get feature matrix\n",
    "X = square.node_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-satellite",
   "metadata": {},
   "source": [
    "# Modeling: GCN_LSTM_WX\n",
    "\n",
    "###### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-simulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the training rate\n",
    "train_rate = 0.8\n",
    "\n",
    "# replace missing values with nans\n",
    "X = np.where(X<0,0,X)\n",
    "\n",
    "# split train/test\n",
    "train_data, test_data = preprocessing.train_test_split(X, train_rate)\n",
    "wx_train_data, wx_test_data = preprocessing.train_test_split(wx_vars_scaled, train_rate)\n",
    "\n",
    "print(\"Train data: \", train_data.shape)\n",
    "print(\"Test data: \", test_data.shape)\n",
    "print(\"WX Train data: \", wx_train_data.shape)\n",
    "print(\"WX Test data: \", wx_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-warning",
   "metadata": {},
   "source": [
    "###### Scale Traffic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-david",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data based on max/min\n",
    "train_scaled, test_scaled = preprocessing.scale_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-individual",
   "metadata": {},
   "source": [
    "##### Set weather parameters for each node of road network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-middle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new train/test variables for wx variables\n",
    "wx_train_data_ = []\n",
    "wx_test_data_ = []\n",
    "\n",
    "# loop thru and assign the wx data to each node\n",
    "for i in range(70):\n",
    "    wx_train_data_.append(wx_train_data.T)\n",
    "    wx_test_data_.append(wx_test_data.T)\n",
    "\n",
    "# convert data to correct shape\n",
    "wx_train_data_ = np.array(wx_train_data_)\n",
    "wx_test_data_ = np.array(wx_test_data_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-baseline",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "## 5-min sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-curtis",
   "metadata": {},
   "source": [
    "###### Pre-process data based on sequence and prediction length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extra-cross",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of timesteps up to the prediction that we will feed to the model (5-minute intervals)\n",
    "seq_len = 12\n",
    "\n",
    "# the amount of time in advance we want to predict (5-minute intervals)\n",
    "pre_len = 1\n",
    "\n",
    "# preprocess traffic data\n",
    "traffic_trainX, trainY, traffic_testX, testY = preprocessing.sequence_data_preparation(\n",
    "    seq_len, pre_len, train_scaled, test_scaled\n",
    ")\n",
    "\n",
    "# preprocessing weather data\n",
    "wx_trainX, wx_trainY, wx_testX, wx_testY = preprocessing.sequence_data_preparation(\n",
    "    seq_len, pre_len, wx_train_data_, wx_test_data_\n",
    ")\n",
    "\n",
    "print(traffic_trainX.shape)\n",
    "print(trainY.shape)\n",
    "print(traffic_testX.shape)\n",
    "print(testY.shape)\n",
    "print(wx_trainX.shape)\n",
    "print(wx_trainY.shape)\n",
    "print(wx_testX.shape)\n",
    "print(wx_testY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-connecticut",
   "metadata": {},
   "source": [
    "##### Combine the weather variables w/ traffic matrix to create feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-fifty",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = np.empty((len(wx_trainX[:,0,0,0]),len(wx_trainX[0,:,0,0]),len(wx_trainX[0,0,:,0]),5) )\n",
    "testX = np.empty((len(wx_testX[:,0,0,0]),len(wx_testX[0,:,0,0]),len(wx_testX[0,0,:,0]),5) )\n",
    "\n",
    "trainX[:,:,:,0] = traffic_trainX\n",
    "trainX[:,:,:,1:5] = wx_trainX\n",
    "testX[:,:,:,0] = traffic_testX\n",
    "testX[:,:,:,1:5] = wx_testX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-analyst",
   "metadata": {},
   "source": [
    "## Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-grill",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_lstm = GCN_LSTM(\n",
    "    seq_len=seq_len,\n",
    "    adj=A,\n",
    "    gc_layer_sizes=[10],\n",
    "    gc_activations=[\"linear\"],\n",
    "    lstm_layer_sizes=[200],\n",
    "    lstm_activations=[\"linear\"],\n",
    "    dropout=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-choir",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build data fusion layer which will merge wx/traffic feature matrix (length 5) into one array to be fed into the t-gcn model\n",
    "input_layer = Input(shape=(70,12,5))\n",
    "layer1 = Dense(1, activation='linear')(input_layer)\n",
    "layer2 = BatchNormalization()(layer1)\n",
    "layer3 = Dropout(0.1)(layer1)\n",
    "output_layer = Reshape((70,12))(layer3)\n",
    "data_fusion_model = Model(input_layer,output_layer)\n",
    "\n",
    "# recall tgcn model\n",
    "x_input, x_output = gcn_lstm.in_out_tensors()\n",
    "tgcn_model = Model(inputs=x_input, outputs=x_output)\n",
    "\n",
    "\n",
    "# of data fusion model feeds into t-gcn\n",
    "output = tgcn_model(data_fusion_model.output)\n",
    "\n",
    "# define entire model \n",
    "tgcn_wx_model = Model(data_fusion_model.input,output, name=\"T-GCN-WX\")\n",
    "tgcn_wx_model.summary()\n",
    "\n",
    "# compile model\n",
    "optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "tgcn_wx_model.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mse\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-kansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = tgcn_wx_model.fit(\n",
    "    x=trainX,\n",
    "    y=trainY,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    verbose=1,\n",
    "    validation_data=(testX,testY)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-toner",
   "metadata": {},
   "outputs": [],
   "source": [
    "sg.utils.plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-singapore",
   "metadata": {},
   "source": [
    "## Save Model Weights (T-GCN-WX: 5-min prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-offer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to folder\n",
    "tgcn_wx_model.save('modeling/models/tgcn_wx-5min')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "described-reduction",
   "metadata": {},
   "source": [
    "## 15-min sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-hurricane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of timesteps up to the prediction that we will feed to the model (5-minute intervals)\n",
    "seq_len = 12\n",
    "\n",
    "# the amount of time in advance we want to predict (5-minute intervals)\n",
    "pre_len = 3\n",
    "\n",
    "# preprocess traffic data\n",
    "traffic_trainX, trainY, traffic_testX, testY = preprocessing.sequence_data_preparation(\n",
    "    seq_len, pre_len, train_scaled, test_scaled\n",
    ")\n",
    "\n",
    "# preprocessing weather data\n",
    "wx_trainX, wx_trainY, wx_testX, wx_testY = preprocessing.sequence_data_preparation(\n",
    "    seq_len, pre_len, wx_train_data_, wx_test_data_\n",
    ")\n",
    "\n",
    "trainX = np.empty((len(wx_trainX[:,0,0,0]),len(wx_trainX[0,:,0,0]),len(wx_trainX[0,0,:,0]),5) )\n",
    "testX = np.empty((len(wx_testX[:,0,0,0]),len(wx_testX[0,:,0,0]),len(wx_testX[0,0,:,0]),5) )\n",
    "\n",
    "trainX[:,:,:,0] = traffic_trainX\n",
    "trainX[:,:,:,1:5] = wx_trainX\n",
    "testX[:,:,:,0] = traffic_testX\n",
    "testX[:,:,:,1:5] = wx_testX\n",
    "\n",
    "gcn_lstm = GCN_LSTM(\n",
    "    seq_len=seq_len,\n",
    "    adj=A,\n",
    "    gc_layer_sizes=[10],\n",
    "    gc_activations=[\"linear\"],\n",
    "    lstm_layer_sizes=[100],\n",
    "    lstm_activations=[\"linear\"],\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# build data fusion layer which will merge wx/traffic feature matrix (length 5) into one array to be fed into the t-gcn model\n",
    "input_layer = Input(shape=(70,12,5))\n",
    "layer1 = Dense(1, activation='linear')(input_layer)\n",
    "layer2 = BatchNormalization()(layer1)\n",
    "layer3 = Dropout(0.1)(layer1)\n",
    "output_layer = Reshape((70,12))(layer3)\n",
    "data_fusion_model = Model(input_layer,output_layer)\n",
    "\n",
    "# recall tgcn model\n",
    "x_input, x_output = gcn_lstm.in_out_tensors()\n",
    "tgcn_model = Model(inputs=x_input, outputs=x_output)\n",
    "\n",
    "\n",
    "# of data fusion model feeds into t-gcn\n",
    "output = tgcn_model(data_fusion_model.output)\n",
    "\n",
    "# define entire model \n",
    "tgcn_wx_model_15 = Model(data_fusion_model.input,output, name=\"T-GCN-WX\")\n",
    "tgcn_wx_model_15.summary()\n",
    "\n",
    "# compile model\n",
    "optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "tgcn_wx_model_15.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mse\"])\n",
    "\n",
    "history_15 = tgcn_wx_model_15.fit(\n",
    "    x=trainX,\n",
    "    y=trainY,\n",
    "    epochs=80,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    verbose=1,\n",
    "    validation_data=(testX,testY)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minor-display",
   "metadata": {},
   "outputs": [],
   "source": [
    "sg.utils.plot_history(history_15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-graph",
   "metadata": {},
   "source": [
    "## Save Model Weights (T-GCN-WX: 15-min prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-vintage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to folder\n",
    "tgcn_wx_model_15.save('modeling/models/tgcn_wx-15min')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-aaron",
   "metadata": {},
   "source": [
    "## 30-min sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-estate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of timesteps up to the prediction that we will feed to the model (5-minute intervals)\n",
    "seq_len = 12\n",
    "\n",
    "# the amount of time in advance we want to predict (5-minute intervals)\n",
    "pre_len = 6\n",
    "\n",
    "# preprocess traffic data\n",
    "traffic_trainX, trainY, traffic_testX, testY = preprocessing.sequence_data_preparation(\n",
    "    seq_len, pre_len, train_scaled, test_scaled\n",
    ")\n",
    "\n",
    "# preprocessing weather data\n",
    "wx_trainX, wx_trainY, wx_testX, wx_testY = preprocessing.sequence_data_preparation(\n",
    "    seq_len, pre_len, wx_train_data_, wx_test_data_\n",
    ")\n",
    "\n",
    "trainX = np.empty((len(wx_trainX[:,0,0,0]),len(wx_trainX[0,:,0,0]),len(wx_trainX[0,0,:,0]),5) )\n",
    "testX = np.empty((len(wx_testX[:,0,0,0]),len(wx_testX[0,:,0,0]),len(wx_testX[0,0,:,0]),5) )\n",
    "\n",
    "trainX[:,:,:,0] = traffic_trainX\n",
    "trainX[:,:,:,1:5] = wx_trainX\n",
    "testX[:,:,:,0] = traffic_testX\n",
    "testX[:,:,:,1:5] = wx_testX\n",
    "\n",
    "gcn_lstm = GCN_LSTM(\n",
    "    seq_len=seq_len,\n",
    "    adj=A,\n",
    "    gc_layer_sizes=[15],\n",
    "    gc_activations=[\"linear\"],\n",
    "    lstm_layer_sizes=[100],\n",
    "    lstm_activations=[\"relu\"],\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# build data fusion layer which will merge wx/traffic feature matrix (length 5) into one array to be fed into the t-gcn model\n",
    "input_layer = Input(shape=(70,12,5))\n",
    "layer1 = Dense(1, activation='linear')(input_layer)\n",
    "layer2 = BatchNormalization()(layer1)\n",
    "layer3 = Dropout(0.5)(layer1)\n",
    "output_layer = Reshape((70,12))(layer3)\n",
    "data_fusion_model = Model(input_layer,output_layer)\n",
    "\n",
    "# recall tgcn model\n",
    "x_input, x_output = gcn_lstm.in_out_tensors()\n",
    "tgcn_model = Model(inputs=x_input, outputs=x_output)\n",
    "\n",
    "\n",
    "# of data fusion model feeds into t-gcn\n",
    "output = tgcn_model(data_fusion_model.output)\n",
    "\n",
    "# define entire model \n",
    "tgcn_wx_model_30 = Model(data_fusion_model.input,output, name=\"T-GCN-WX\")\n",
    "tgcn_wx_model_30.summary()\n",
    "\n",
    "# compile model\n",
    "optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "tgcn_wx_model_30.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mse\"])\n",
    "\n",
    "history_30 = tgcn_wx_model_30.fit(\n",
    "    x=trainX,\n",
    "    y=trainY,\n",
    "    epochs=80,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    verbose=1,\n",
    "    validation_data=(testX,testY)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-issue",
   "metadata": {},
   "outputs": [],
   "source": [
    "sg.utils.plot_history(history_30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-conjunction",
   "metadata": {},
   "source": [
    "## Save Model Weights (T-GCN-WX: 30-min prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-salmon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to folder\n",
    "tgcn_wx_model_30.save('modeling/models/tgcn_wx-30min')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-workshop",
   "metadata": {},
   "source": [
    "## 60-min sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-alliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of timesteps up to the prediction that we will feed to the model (5-minute intervals)\n",
    "seq_len = 12\n",
    "\n",
    "# the amount of time in advance we want to predict (5-minute intervals)\n",
    "pre_len = 12\n",
    "\n",
    "# preprocess traffic data\n",
    "traffic_trainX, trainY, traffic_testX, testY = preprocessing.sequence_data_preparation(\n",
    "    seq_len, pre_len, train_scaled, test_scaled\n",
    ")\n",
    "\n",
    "# preprocessing weather data\n",
    "wx_trainX, wx_trainY, wx_testX, wx_testY = preprocessing.sequence_data_preparation(\n",
    "    seq_len, pre_len, wx_train_data_, wx_test_data_\n",
    ")\n",
    "\n",
    "trainX = np.empty((len(wx_trainX[:,0,0,0]),len(wx_trainX[0,:,0,0]),len(wx_trainX[0,0,:,0]),5) )\n",
    "testX = np.empty((len(wx_testX[:,0,0,0]),len(wx_testX[0,:,0,0]),len(wx_testX[0,0,:,0]),5) )\n",
    "\n",
    "trainX[:,:,:,0] = traffic_trainX\n",
    "trainX[:,:,:,1:5] = wx_trainX\n",
    "testX[:,:,:,0] = traffic_testX\n",
    "testX[:,:,:,1:5] = wx_testX\n",
    "\n",
    "gcn_lstm = GCN_LSTM(\n",
    "    seq_len=seq_len,\n",
    "    adj=A,\n",
    "    gc_layer_sizes=[15],\n",
    "    gc_activations=[\"linear\"],\n",
    "    lstm_layer_sizes=[100],\n",
    "    lstm_activations=[\"relu\"],\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# build data fusion layer which will merge wx/traffic feature matrix (length 5) into one array to be fed into the t-gcn model\n",
    "input_layer = Input(shape=(70,12,5))\n",
    "layer1 = Dense(1, activation='linear')(input_layer)\n",
    "layer2 = BatchNormalization()(layer1)\n",
    "layer3 = Dropout(0.5)(layer1)\n",
    "output_layer = Reshape((70,12))(layer3)\n",
    "data_fusion_model = Model(input_layer,output_layer)\n",
    "\n",
    "# recall tgcn model\n",
    "x_input, x_output = gcn_lstm.in_out_tensors()\n",
    "tgcn_model = Model(inputs=x_input, outputs=x_output)\n",
    "\n",
    "\n",
    "# of data fusion model feeds into t-gcn\n",
    "output = tgcn_model(data_fusion_model.output)\n",
    "\n",
    "# define entire model \n",
    "tgcn_wx_model_60 = Model(data_fusion_model.input,output, name=\"T-GCN-WX\")\n",
    "tgcn_wx_model_60.summary()\n",
    "\n",
    "# compile model\n",
    "optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "tgcn_wx_model_60.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mse\"])\n",
    "\n",
    "history_60 = tgcn_wx_model_60.fit(\n",
    "    x=trainX,\n",
    "    y=trainY,\n",
    "    epochs=80,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    verbose=1,\n",
    "    validation_data=(testX,testY)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-lobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "sg.utils.plot_history(history_60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-eight",
   "metadata": {},
   "source": [
    "## Save Model Weights (T-GCN-WX: 60-min prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-refrigerator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to folder\n",
    "tgcn_wx_model_60.save('modeling/models/tgcn_wx-60min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-softball",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = tgcn_wx_model_30.predict(testX)\n",
    "\n",
    "## Rescale values\n",
    "max_speed = train_data.max()\n",
    "min_speed = train_data.min()\n",
    "\n",
    "## actual train and test values\n",
    "test_rescref = np.array(testY * max_speed)\n",
    "\n",
    "## Rescale model predicted values\n",
    "test_rescpred = np.array((yhat) * max_speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "determined-block",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_performance.spatio_temporal_traffic_model()\n",
    "model.fit(traffic_trainX,trainY,traffic_testX,testY)\n",
    "model.plot_specific_road_speed_performance(\"Linear Regression\",time=time,road_seg_num=20,window=12*12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-floating",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_specific_road_speed_performance(\"GCN-LSTM\",time=time, road_seg_num=20,predictions=yhat.T,true=testY,window=12*12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-patio",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_mean_road_speed_performance(\"Linear Regression\",time=time,window=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-brazil",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_mean_road_speed_performance(\"GCN-LSTM\",time=time,predictions=yhat.T,true=testY,window=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-montgomery",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-dominican",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
